{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db558a1-c764-4549-8e12-9290da8fda3e",
   "metadata": {},
   "source": [
    "# 1. The Norwegian UD treebank: A Nynorsk text dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a9915-7b1f-4a2b-8f84-6ed556f090b1",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The Norwegian UD treebank is based on the Nynorsk section of the Norwegian\n",
    "Dependency Treebank (NDT), which is a syntactic treebank of Norwegian. \n",
    "NDT has been automatically converted to the UD\n",
    "scheme by Lilja Øvrelid at the University of Oslo.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "NDT was developed 2011-2014 at the National Library of Norway in collaboration\n",
    "with the Text Laboratory and the Department of Informatics at the\n",
    "University of Oslo.\n",
    "NDT contains around 300,000 tokens taken from a variety of genres.\n",
    "The treebank texts have been manually annotated for morphosyntactic\n",
    "information. The morphological annotation mainly follows\n",
    "the [Oslo-Bergen Tagger](http://tekstlab.uio.no/obt-ny/).  The syntactic\n",
    "annotation follows, to a large extent, the Norwegian Reference\n",
    "Grammar, as well as a dependency annotation scheme formulated at the\n",
    "outset of the annotation project and iteratively refined throughout\n",
    "the construction of the treebank. For more information, see the\n",
    "references below.\n",
    "\n",
    "## Run the code below to load the dataset and words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ec4b8-da4b-4be4-8957-6479c6206838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_data\n",
    "\n",
    "nynorskCorpus, nynorskTokens = read_data(\"../datasets/UD_Norwegian-Nynorsk-master/no_nynorsk-ud-train.conllu\")\n",
    "# nynorskCorpus, nynorskTokens = read_data(\"../datasets/UD_Norwegian-Nynorsk-master/no_nynorsk-ud-test.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0629088-61c3-449d-9874-6316d801e6a7",
   "metadata": {},
   "source": [
    "## 1.1 Visualize the text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c883e2c3-f3ce-4831-aa32-5f96f90497f4",
   "metadata": {},
   "source": [
    "Let's take a closer look at all the words that are available in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dde672-4089-47da-8d40-ec65017fc0b3",
   "metadata": {},
   "source": [
    "### Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b749f9-7a73-4741-82b5-611a238a9e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(\" \".join(nynorskTokens))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be242b-20ec-4b2a-b9ed-8c247d85024c",
   "metadata": {},
   "source": [
    "### Frequency of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca09d81-c17a-4c6d-a74b-2158a62d1207",
   "metadata": {},
   "source": [
    "Check what are the most frequent words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7e4bc-7657-43d6-8fc9-682779fd4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import FreqDist\n",
    "import seaborn as sns\n",
    "\n",
    "freq_dist = FreqDist(nynorskTokens)\n",
    "df_freq_dist = pd.DataFrame(list(freq_dist.items()), columns=[\"Token\", \"Frequency\"])\n",
    "df_freq_dist = df_freq_dist.sort_values(by=\"Frequency\", ascending=False)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Token\", y=\"Frequency\", data=df_freq_dist.head(20))\n",
    "plt.title(\"Top 20 Most Frequent Tokens\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0ec2c-f336-447a-9e10-fcb333cddf22",
   "metadata": {},
   "source": [
    "### Word lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b7b92-4f78-46b6-93cf-a0cc5fb77464",
   "metadata": {},
   "source": [
    "How long are the words in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e912e19-f2bf-4e75-887e-657f9167a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(nynorskTokens)), [len(token) for token in nynorskTokens], alpha=0.5)\n",
    "plt.title(\"Scatter Plot of Word Lengths\")\n",
    "plt.xlabel(\"Token Index\")\n",
    "plt.ylabel(\"Word Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7768a-1bbd-418f-80f0-61c4e9ca7137",
   "metadata": {},
   "source": [
    "# 2. Let's Train a simple language model for sentence generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ffc06-8534-4429-b92b-869295d9e40b",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; flex-direction: row;\">\n",
    "    <img src=\"resources/ngram.png\" alt=\"Image 1\" width=\"500px\" height=\"150px\" style=\"margin-left: 300px;\">\n",
    "</div>\n",
    "\n",
    "N-gram models serve as fundamental language models for text generation, providing a probabilistic framework to capture the structure and patterns within a sequence of words. In the context of natural language processing, an n-gram refers to a contiguous sequence of n items, typically words. N-gram models estimate the likelihood of a word based on its context—the preceding n-1 words. The key assumption is that the probability of a word depends only on a limited history of preceding words, making computation more feasible. These models offer simplicity and efficiency, making them foundational in various language processing tasks, including text generation, machine translation, and speech recognition. While n-gram models exhibit effectiveness in capturing local dependencies, they may struggle with long-range dependencies and fail to capture the broader semantic context present in more advanced models like transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d641b9e-5cdc-47ee-875d-0b28fe3ef559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a simple Bigram model from the training data loaded above.\n",
    "\n",
    "from utils import train_ngram_model\n",
    "ngram_model = train_ngram_model(nynorskTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30631956905ae116",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate text starting with a seed\n",
    "from utils import generate_text\n",
    "\n",
    "seed = 'Hallo hvordan går det' # The initial prompt for generating a sentence.\n",
    "generated_text = generate_text(ngram_model , seed, length=50) # generate sample text using the given function.\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f378c-ec74-42e5-90ba-7214dbf758cc",
   "metadata": {},
   "source": [
    "# 3. Large Language Models\n",
    "\n",
    "<div style=\"display: flex; flex-direction: row;\">\n",
    "    <img src=\"resources/lstm.png\" alt=\"Image 1\" width=\"500px\" height=\"150px\" style=\"margin-right: 10px;\">\n",
    "    <img src=\"resources/transformer.png\" alt=\"Image 2\" width=\"500px\" height=\"100\" style=\"margin-right: 10px;\">\n",
    "</div>\n",
    "\n",
    "Neural language models, such as those based on recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and transformer architectures, have shown superior performance compared to n-gram models in various natural language processing (NLP) tasks. Here are some reasons why neural language models are generally considered better:\n",
    "\n",
    "***Long-range Dependencies:***\n",
    "\n",
    "N-gram models capture dependencies up to a fixed number of preceding words (the \"n\" in n-gram). Neural language models, especially transformer architectures, can capture long-range dependencies in a sequence of words, allowing them to model more complex relationships.\n",
    "\n",
    "***Parameter Efficiency:***\n",
    "\n",
    "Neural language models can efficiently represent and learn from large amounts of data with relatively fewer parameters compared to n-gram models. This is crucial in dealing with the vast amount of information present in natural language.\n",
    "\n",
    "***Continuous Embeddings:***\n",
    "\n",
    "Neural models represent words as continuous embeddings in a high-dimensional space. This continuous representation allows the model to capture semantic relationships between words, which is challenging for discrete representations used in n-grams.\n",
    "\n",
    "***Generalization:***\n",
    "\n",
    "Neural models generalize better to unseen or rare words because they learn continuous representations that can capture similarities between words. N-gram models struggle with out-of-vocabulary words and may not generalize well to unseen contexts.\n",
    "\n",
    "***Adaptability to Task Complexity:***\n",
    "\n",
    "Neural models can adapt to the complexity of different NLP tasks by fine-tuning or adjusting hyperparameters. N-gram models have limited capacity to adapt to different tasks without modifying the n-gram order, which may not be practical.\n",
    "\n",
    "***Handling Variable-Length Contexts:***\n",
    "\n",
    "Neural models can handle variable-length contexts, making them more flexible in processing sequences of different lengths. In contrast, n-gram models require fixed-length contexts, which can be limiting.\n",
    "\n",
    "***Contextual Information:***\n",
    "\n",
    "Models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) consider contextual information by processing the entire sequence bidirectionally or unidirectionally. This allows them to capture richer context for each word.\n",
    "\n",
    "***State-of-the-Art Performance:***\n",
    "\n",
    "Neural language models have achieved state-of-the-art performance on various NLP benchmarks, including tasks such as language modeling, machine translation, text summarization, and sentiment analysis.\n",
    "Despite the advantages of neural language models, n-gram models can still be useful in certain scenarios, especially when dealing with limited resources or when a simple model is sufficient for the task at hand. The choice between n-gram models and neural language models often depends on the specific requirements of the task, the amount of available data, and computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0fda7-d2b2-4e6e-bbef-2090ef7d629e",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Check out this link for a tokenization playground for ChatGPT: https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573cb499-8512-420c-83f5-9ad14c9a55ea",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; flex-direction: row;\">\n",
    "    <img src=\"resources/tokenizer.png\" alt=\"Image 1\" width=\"700px\" height=\"300px\" style=\"margin-left: 200px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ce552-dcc2-467a-8e4e-c758205e557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import tokenize_text\n",
    "tokens = tokenize_text(\"Supercalifragilisticexpialidocious\")\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "tokens = tokenize_text(\"ordbokstavrimkonkurranse\")\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "tokens = tokenize_text(\"Enter your word here\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9097f-aabb-48cb-bc72-670214b01467",
   "metadata": {},
   "source": [
    "## Running an LLM on local host requires lots of resources and takes time, So in the next section we will use OpenAI API for remote access to a GPT model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
