{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b59266",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Initialisation\n",
    "For the examples in the notebook to work as intended, the openai and dotenv(python-dotenv) libraries need to be installed in the environment where the notebook is started. If necessary, run `!pip install <library>` in a code cell to install a missing library.\n",
    "\n",
    "Beforre doing something interesting, we need to import some libraries and methods, and we need to provide a valid openai key (below, this is read from a local .env - file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5307aae-1834-4027-9c19-5a90ffdd6e1f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries used in this chapter\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# useful pandas - setting\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a3aed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# reading openai-key and initialising client\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # reading from local .env - file\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "client=OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85065ac-481b-4a06-80e1-243b094f5a0d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Basic usage of the ChatCompletions endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607e84a-32d9-4922-8e3a-1835807d0675",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.1 Calling the API and getting a response... \n",
    "To answer a natural language question using the API, we need to...  \n",
    " - initiate the client\n",
    " - choose a model using the \"model\" parameter (see [OpenAI model docs](https://platform.openai.com/docs/models/overview) for updated details)\n",
    " - intitiate the interaction using the \"message\" parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a4509-8fa1-41cc-aecb-bf8636a32206",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initiating the client and making a first request using the \"message\" parameter\n",
    "\n",
    "# initiating the interaction\n",
    "messages = [  \n",
    "{\"role\": \"system\", \"content\": \"You are a helpful tourist information agent in Bergen, Norway.\"},    \n",
    "{\"role\": \"user\", \"content\": \"Hvor finner jeg det berømte akvariet?\"}  \n",
    "] \n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "pprint(response.choices[0].message.content, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00f4484-46fc-44ab-9daf-31800be71201",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.2 Useful chat.completions.create - parameters: *max_tokens, temperature, top_p and n*   \n",
    "***temperature***: adjusts the \"creativity\" of the responses: a higher temperature gives more imaginative and varied responses, but also a higher frequency of errors and hallucinations. Default=1, max=2, min=0. To get repeatable, strictly factual answers use *temperature*=0.   \n",
    "***top_p***: cutoff probablity value for predicted tokens. Using a lower value means using a stricter \"filter\" when adding new elements in the response, resulting in shorter, less varied and more to-the-point answers. Default=1. \n",
    "Note: openAI recommends to use only use one of the *temperature* and *top_p* parameters for tuning, and leave the other at default (=1).  \n",
    "***max_tokens***: limit for the number of tokens used (prompt and response in total).  \n",
    "***n***: the number of responses returned. Generating several responses to the same query can in some situations be used as input to multi-step processes selecting the \"best\" response among several (according to some evaluation criteria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0150d2-49db-4fd3-8ca1-9ddff3494221",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# using top_p or temperature to adjust \"creativity\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    top_p=1\n",
    ")\n",
    "pprint(response.choices[0].message.content, width=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741015e2-f8c7-4c09-8d9d-9b3eb08dcfe6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generating several responses - try adjusting the temperature\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=1,\n",
    "    n=3\n",
    ")\n",
    "for c in response.choices:\n",
    "    pprint('Svar nr ' + str(c.index+1) + \": \"+  c.message.content, width=80)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680e76c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# temperature=0 does not necessarily eliminate Hallucinations\n",
    "# prompt adjustment: You are very factual. If you do not know something say you do not know.\n",
    "\n",
    "messages = [  \n",
    "{\"role\": \"system\", \"content\": \"You are a tourist information agent in Bergen, Norway.\"},    \n",
    "{\"role\": \"user\", \"content\": \"Hvor finner jeg kontoret til Tryg forsikring?\"}  \n",
    "] \n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "pprint(response.choices[0].message.content, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a779baa0-a63c-4476-95fe-b87c2afba4a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3 Some additional chat.completions.create - parameters \n",
    "***frequency_penalty***: Reduces the probability of adding tokens accoording to the frequency of which they appear in the preceding text. Scale: -2 to 2, a higher positive value means a stricter \"penalty\" for high frequency.<br/>\n",
    "***presence_penalty***: Reduces the probability of adding tokens if they occur in the preceding text. Scale: -2 to 2, a higher positive value indicates a stricter \"penalty\" for earlier occurence.<br/>\n",
    "***stop***: kan (optionally) be used to let selected words og word combinations terminate the answer, if they occur.<br/>\n",
    "***seed***: (beta-funksjonalitet) supply a seed value to generate a deterministic sample (however this functionality is in beta, and a deterministic sample is not guaranteed).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d474379-cdda-40e5-962f-ab42e2051dde",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. The chat completions - object (the response you get when calling chat.completions.create) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9984f1a-5fae-48c9-ad64-c566e798db67",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The chat completions - object contains a lot of elements, however many are just relatively uninteresting reference information that one would normally ignore. See the official [API-documentation](https://platform.openai.com/docs/api-reference/chat/object) for a general (and up-to-date) reference.\n",
    "\n",
    "The most interesting parts here are the *choices* and to some extent *usage* lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed3b4f0-159b-4ff3-ad3a-71946b9d6eef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"resources/chat_completion.png\" alt=\"OpenAI chatCompletion\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b42230-bbd3-411b-ad1f-0850904458b8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The **usage** (list) provides a count of tokens used in the prompt, in the response, and in total. These figures can be useful for cost control and in situations when context token limitations are an issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc175ba-c24d-4eb4-85e5-1f5ce7f82081",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inspecting the \"usage\" list\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    top_p=1\n",
    ")\n",
    "pprint(dict(response.usage))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7937b242-92db-4cc3-afbf-29d0d8c69537",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The ***choices*** list contains the responses generated by the model.<br/> \n",
    "Each ***choices[index]***- list element in turn contains the elements: \n",
    " - finish_reason: indicates why the reponse generation was terminated - useful for identifying technical issues like problems with context length\n",
    " - logprobs: gives probability values for each token in the generated response\n",
    " - message (liste): the chat messages and some details on how each response has been generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966eb294-c2a8-4bbf-af6e-0b3f2bd31b60",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inspecting an element in the \"choices\" list\n",
    "pprint(dict(response.choices[0]), depth=1, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631d28c-6225-4e3b-b27d-a6f854da4576",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The ***choices[index].message*** - list contains the actual response text as well as some additional information concerning how the response was generated:  \n",
    "- role: the role providing the response (normally \"assistant\") \n",
    "- content: the response \n",
    "- tool_calls: describing any tool calls (see below) the agent has done to produce the response\n",
    "\n",
    "Note that the content of \"messages\" in the call to chat.completions.create is NOT included in the response, so if you want to keep track of a several-step conversation you will need to implement some way of storing the conversation history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f95c9-368f-432c-af1f-bee38f2e42a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inspecting the \"message\" list for a \"choices\" list element\n",
    "pprint(dict(response.choices[0].message), depth=1, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ffbb28-1d6f-4868-9277-e90486530513",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Tools og Tool Calls - additional super-powers for the ChatCompletions API \n",
    "Using the ChatCompletions API, it is possible to give the LLM access to additional \"tools\" to produce responses - for example, it can be supplied with updated information from the web. \n",
    "\n",
    "The API cannot call external systems on its own, but it can be provided with a formal description of available functions - tools - that it can use. When processing a request, the LLM then decides whether to use a tool. If it wants to use a tool, it returns a function call in the specified format. Actually calling the function in question has to be done in the application calling the API (with the appropriate caution and security checks).\n",
    "\n",
    "The typical steps for using function calling in an application can be summarized as follows:\n",
    "1. Call the LLM model with the user query and a set of functions defined in the functions parameter.\n",
    "2. Based on the query and the function description(s), the model choose whether to call one or more functions. If using a function, the message content will be a JSON object matching the function description.\n",
    "3. Parse the returned string into JSON in your application code, and call your function with the provided arguments.\n",
    "4. Call the LLM model again by appending the function response as a new message (role=\"tool\") and finally let the model summarize the results back to the user.\n",
    "\n",
    "A simple (and somewhat clumsy) practical example of this approach in action can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f21d74-9b84-48ca-ab17-930692e6cf5d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# a dummy function to call\n",
    "\n",
    "# A hard coded \"weather app\" function - could be replaced with external API call or similar\n",
    "def get_current_weather(location):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if \"bergen\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Bergen\", \"temperature\": \"-10\", \"unit\": \"celcius\", \"weather\": \"heavy snow\"})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"85\", \"unit\": \"fahrenheit\", \"weather\": \"sunny\"})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\", \"weather\": \"cloudy\"})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "\n",
    "# Also, we need a json description of the \"weather app\" function: name, free-text description and parameters\n",
    "tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc836cc-1690-4fd9-90a7-8832e01afe9a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run_conversation: helper function to answer a random user query, potentially using a tool\n",
    "\n",
    "def run_conversation(query):\n",
    "      # Step 1: initiate the conversation - send the query and available functions to the model\n",
    "    messages = [{\"role\": \"user\", \"content\": query}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=messages,\n",
    "        temperature=1.5,\n",
    "        tools=tools, # function description \n",
    "        tool_choice=\"auto\",  # auto is the default, can be overrun\n",
    "    )\n",
    "\n",
    "    # Step 2: store the initial response message, and check if the model wanted to call a function\n",
    "    response_message = response.choices[0].message\n",
    "    messages.append(response_message)\n",
    "    tool_calls = response_message.tool_calls\n",
    "\n",
    "    if tool_calls:\n",
    "        # Step 3 (optional): call the function(s)\n",
    "        # There is only one function in this example, but you can have multiple\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        } \n",
    "        \n",
    "        for tool_call in tool_calls:\n",
    "            # call chosen function, with arguments as specified by the model\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments) \n",
    "            function_response = function_to_call(\n",
    "                location=function_args.get(\"location\")\n",
    "            ) \n",
    "            \n",
    "            # extend conversation with function response\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            ) \n",
    "            \n",
    "        # Step 4 (optional): send data about each function call and response back to the model;\n",
    "        # get a new response from the model where it can see the function response\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=messages,\n",
    "        )  \n",
    "        messages.append(second_response.choices[0].message)  \n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f72ff0-1ece-40a8-aede-d92fa1a0160d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# function calling in action\n",
    "\n",
    "# Using the output from run_conversation, we can inspect messages generated for different queries.\n",
    "# examples: weather in Bergen, The Smiths members, Python lists...\n",
    "# Generally the agent is quite \"trigger happy\" when deciding whether to use tools.\n",
    "\n",
    "test_messages=run_conversation(\"Who are the members of the Smiths?\")\n",
    "\n",
    "for msg in test_messages: \n",
    "    pprint(dict(msg))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ad010-21f9-419a-b5dc-1904d7f73b0a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. The Embeddings endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ace7a8-2c34-4d9a-ba0e-87dedc12c57f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<center><img src=\"resources/image resolution.jpg\" alt=\"image resolution\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d6754f-bc70-4a12-b732-20f2ddecefc4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "OpenAI provides a separate endpoint for converting text input to so-called embeddings, which are (long) vectors of floats. Embeddings can be understood as a dimensional reduction technique for text data and are typically used for comparing text elements for similarity in a quick and computationally cheap way. \n",
    "\n",
    "In the context of LLM applications, embeddings are often used as a tool to build relevant and \"dense\" context information for LLM prompts. Using embeddings and various numerical similarity measures, text parts from large volumes of background information can be ranked according to relevance for a given user query. \n",
    "\n",
    "When using the embeddings endpoint, the maximum amount of input tokens allowed and the dimensionality of the returned vectors both depend on the selected embedding model. Using the standard choice of embedding model - *text-embedding-3-small* - the input text chunks can be up to 8191 tokens long and the returned vectors are 1536-dimensional, ie they always contain 1536 numbers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb2c22-5bcc-41dc-a47d-956e395d383d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1 A simple embeddings example\n",
    "Below we use a small set of text samples to illustrate some features of embeddings and the OpenAI embeddings endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360b444-be14-4d5c-b4f0-52f4b560d790",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4.1.1 Loading and inspecting text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91d554-fa95-4870-9ecb-892ed900bfa4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read and print prepared data set with text samples\n",
    "\n",
    "# helper function to count tokens\n",
    "def token_count(text, encoding_model):\n",
    "    encoding=tiktoken.get_encoding(encoding_model)\n",
    "    n_tokens = len(encoding.encode(text))\n",
    "    return n_tokens\n",
    "\n",
    "# read text samples from csv, and add a token count column \n",
    "df_text = pd.read_csv('data/text_samples_mat.csv', header=0, sep=';')\n",
    "df_text['n_tokens']=df_text.apply(lambda row: token_count(row['quote_text'], \"cl100k_base\"), axis=1)\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da793f6-c0f0-4426-a625-e41292834e7f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inspection a single quote\n",
    "print(df_text.loc[2,'quote_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ce634-e7b6-4d0e-b855-def23539662b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "scrolled": true
   },
   "source": [
    "#### 4.1.2 Creating embeddings using the API\n",
    "Using *embeddings.create* to produce embeddings is rather straightforward - here we use a single string, but the API also accepts arrays of strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c520da-b3ff-4c05-9c26-8e18c8bb386a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create and inspect embedding\n",
    "\n",
    "# embedding helper function - returns embedding using selected model\n",
    "def embed_helper(AIclient, text, model_name):\n",
    "    embedding=client.embeddings.create(\n",
    "          model=model_name,\n",
    "          input=text,\n",
    "          encoding_format=\"float\")\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9403ef28-5802-4b3f-8e11-4effe94396d2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# produce embedding using the API\n",
    "test_embedding=embed_helper(client, df_text.loc[2,'quote_text'], \"text-embedding-3-small\")\n",
    "\n",
    "# checking dimensionality and inspecting the \"raw\" embedding ouput:\n",
    "print('No of elements: '+str(len(test_embedding.data[0].embedding))) \n",
    "print(test_embedding.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681851a-c785-4825-8d59-2e2ef1a3b91d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# simple visualisation of embedding vectors\n",
    "\n",
    "# visualisation helper function\n",
    "def barplot_embedding(embedding, label_list):\n",
    "    sns.heatmap(np.array(embedding.data[0].embedding).reshape(-1,1536), cmap=\"Greys\", center=0, square=False, xticklabels=False, cbar=False)\n",
    "    plt.gcf().set_size_inches(13,1)\n",
    "    plt.yticks([0.5], labels=[label_list])\n",
    "    plt.show()\n",
    "\n",
    "# barplot for embedding selected quotes\n",
    "test_embedding=embed_helper(client, \"\", \"text-embedding-3-small\")\n",
    "barplot_embedding(test_embedding, \"empty string\")\n",
    "\n",
    "test_embedding=embed_helper(client, df_text.loc[0,'quote_text'], \"text-embedding-3-small\")\n",
    "barplot_embedding(test_embedding, df_text.loc[0,'title'])\n",
    "\n",
    "test_embedding=embed_helper(client, df_text.loc[1,'quote_text'], \"text-embedding-3-small\")\n",
    "barplot_embedding(test_embedding, df_text.loc[1,'title'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68758f-c6bd-4b13-97c2-029f026be20a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 4.1.3 Ranking text elements by similarity \n",
    "Using a vector similarity measure, we can rank the quotes in our \"library\" for similariy with a given input statement. \n",
    "\n",
    "Note that mixing languages in these comparisons works technically, but can give strange or less precise results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd69061-2183-4b4e-9e15-c2d48544495b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions for text comparison\n",
    "\n",
    "# naive helper function to compute cosine similarity between two strings\n",
    "def similarity_helper(AIclient, text_1, text_2, embed_model):\n",
    "    text_1_embedded_np=np.array(embed_helper(AIclient, text_1, embed_model).data[0].embedding).reshape(1,-1)\n",
    "    text_2_embedded_np=np.array(embed_helper(AIclient, text_2, embed_model).data[0].embedding).reshape(1,-1)\n",
    "    similarity=cosine_similarity(text_1_embedded_np, text_2_embedded_np)[0,0]\n",
    "    return similarity\n",
    "\n",
    "# helper function to compute similarity of data frame column to an input string\n",
    "def df_add_similarity(AIclient, df_text, df_text_column, input_text, embed_model):   \n",
    "    df_text['input_similarity']=df_text.apply(lambda row: similarity_helper(AIclient, row[df_text_column], input_text, embed_model), axis=1)\n",
    "    return df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d9897-5013-4e96-98f9-8a5df329890f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ranking text elements in a DataFrame according to similarity with a given input string\n",
    "\n",
    "# define input string here (reference for comparisons):\n",
    "# input_text=\"Gourmetpølser har blitt vår nye nasjonalrett, jubler Pølsens Venner\"\n",
    "input_text=df_text.loc[1, 'quote_text']\n",
    "\n",
    "# include reference text in DataFrame \n",
    "df_input=pd.DataFrame(data={'quote_id':100, 'quote_name':'Input', 'author': 'user', 'title':'Input' , 'quote_text': input_text}, index=[10])\n",
    "df_total=pd.concat([df_text, df_input])\n",
    "\n",
    "# calculate similarity\n",
    "df_add_similarity(client, df_total, 'quote_text', input_text, \"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50362f11-3db4-45d1-b9c1-a27f081f8a3d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# creating a simple similarity illustration\n",
    "values=list(df_total.sort_values('input_similarity', ascending=True)['input_similarity'])\n",
    "names=list(df_total.sort_values('input_similarity', ascending=True)[\"quote_text\"].str[:50])\n",
    "color_labels=list(df_total.sort_values('input_similarity', ascending=True)['author'])\n",
    "\n",
    "cmap='tab20'\n",
    "color_map = plt.colormaps[cmap].resampled(20)\n",
    "color_label_set=list(set(color_labels))\n",
    "colors=list()\n",
    "for c in color_labels:\n",
    "    colors.append(color_map(color_label_set.index(c)/len(color_label_set)))\n",
    "                  \n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.barh(names, values, color=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf571f-fb17-4653-ab35-191a1f98abbc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To get a better feel for the possibilities and limitations of embeddings, feel free to repeat and expand on the example with your own quote data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d14ea-7079-475f-be0b-45495dbd6f59",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Moderations\n",
    "\n",
    "The Moderations endpoint scores input text towards a range of different types of unwanted or inappropriate content, and can be used for basic moderation of chat dialogues. In addition to score values, the API also flags input scoring above a treshold value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd71d28-8178-415e-ac6c-828f8e22ad1e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "moderation = client.moderations.create(input=\"Jeg hater alle 50-åringer fra Kolbotn og vil flå dem levende\")\n",
    "pprint(dict(moderation.results[0].category_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6050861-1150-4b26-89d5-a350065ee9a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
