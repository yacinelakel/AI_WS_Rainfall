{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79b3ff6d-4b5a-41f9-8fb7-1b946c1b027d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"display: flex;  height: 300px;\">\n",
    "    <img src=\"resources/langchain.jpeg\"  style=\"margin-left:auto; margin-right:auto\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45ee939-6547-4886-b169-5717b227e833",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# How to efficienty develop software using LLMs\n",
    "***LangChain*** is a framework designed to simplify the creation of applications using large language models (LLMs). It is a language model integration framework that can be used for various purposes such as document analysis and summarization, chatbots, and code analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3465a852-2a5c-44a7-9fd2-fbad9e01b2f6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"display: flex;  height: 600px;\">\n",
    "    <img src=\"resources/langchain_components.png\"  style=\"margin-left:auto; margin-right:auto\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd8cf8-9460-4945-9a6a-9481d6ba05ae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Important**: Run the cell below to load the OpenAI API key for the rest of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d22b67-201f-4679-8ebc-321d92df175f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Chains\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4337ebc0-43ac-4e6c-92a0-2850d90da33d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Why the name LangChain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eff94d-8295-4a3c-a03d-840d26cc63ad",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"display: flex;  height: 80px;\">\n",
    "    <img src=\"resources/simple_chain.png\"  style=\"margin-left:auto; margin-right:auto\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df8313-c9ce-420d-9925-2c8801065f69",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Each operation on the output or input to an LLM is considered a Chain step. The main purpose of LangChain is the ability to \"Chain\" multiple chain steps together and create a complex application which needs multiple operations or calls to different language models. LangChain provides a simple and easy interface which enables one to build such applications and logic. Building such applications is also possible without using LangChain, however with huge applications, the size of the code and maintenance of them could get out of hand quickly. Therefore, using LangChain and its provided tools is a recommended framework for LLM application development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1be29d-9a39-4d91-a1e6-13313d6e7264",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Creat a simple chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d133e07e-9482-4dc8-b7a9-b668cc612b26",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A simple chain can be a simple step of calling an LLM with a prompt and asking for a specific output. Langchain provides a simple and efficient interface called **L**ang**C**hain **E**xpression **L**anguage (***LCEL***) which simplifies the process of creating chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8f27e-7342-4dc3-96e8-a3b98c169e42",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Fortell meg om {topic}\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.4)\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Chain definition is the pipline of putting all the above elements together like below:\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c29c1b-5ceb-4407-8374-a1a0aff23abc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can then call the chain with a custom input variable using the <code>invoke</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf45b22-2ddd-4f1c-b6e0-280f32593973",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(chain.invoke({\"topic\":\"Sverige\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e33b8-7371-4af9-b9c4-fac78ff97ded",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**It might take some time for the output to be generated So maybe you want to try it like below:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f32b6-8ca2-4b72-b805-c1b307cde8c2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for chunk in chain.stream({\"topic\":\"Bergen\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd27502-7318-4bbe-a97c-cb11dfcd9882",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Langchain provides useful tools out of the box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1432f-9b78-43ea-b2fa-61382f66c740",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Many of the previously seen functionalities are already provided in an easy and accessible way by LangChain. So you can develop your applications fast and easy just by calling these functions. Here is an example of text tagging and extraction using OpenAI functions and LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237b442-aa92-4d10-af22-96a271d9c151",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. First create a class that contains the information you want to extract or tags you want to assign to the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5ebb2-173f-481e-83de-5da090f4f989",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Tagging(BaseModel):\n",
    "    \"\"\"Tag the piece of text with particular info.\"\"\"\n",
    "    sentiment: str = Field(description=\"sentiment of text, should be `pos`, `neg`, or `neutral`\")\n",
    "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0454b03-108e-42d2-84da-d577531163fe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Next you can create a tagging chain using the `.bind()` method in model like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4243cc-d12f-472a-a441-05f80cb638c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "# Define the LLM with temprature \n",
    "model = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Create the prompt for the language model.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Think carefully, and tag the sentence\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Convert the Tagging class to OpenAI function format\n",
    "tagging_functions = [convert_pydantic_to_openai_function(Tagging)] \n",
    "\n",
    "# Add the function to the model using the .bind() method\n",
    "model_with_functions = model.bind(\n",
    "    functions=tagging_functions,\n",
    "    function_call={\"name\": \"Tagging\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a627d6ce-debe-4d0b-846d-c0a7de5b0bca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check the description of the model with binded function.\n",
    "model_with_functions.kwargs['functions']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1dd91d-9092-453a-9fb1-22a7d33c483d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Create a chain with the model above and the given extraction prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7688e54f-01a1-4def-884a-8ad4503a1c43",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6118f4a0-c6af-4107-a732-c38685098e6e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. Test out the model with some sample sentences in different languages. (Can get creative here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c79e1b2-a798-4b42-9a92-f047892a935d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tagging_chain.invoke({\"input\": \"I love langchain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b0b18-b794-4244-8eb9-3d03ccb67119",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tagging_chain.invoke({\"input\": \"til helvete med Langchain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a44fd-6b5a-4db7-a739-e1a04dbb4196",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tagging_chain.invoke({\"input\": \"qué es langchain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b16d01-1d39-47bd-b037-fb4c3b8b608e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Try is with your own sample sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c2d597-8f54-4d5b-90dc-70846c558d56",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tagging_chain.invoke({\"input\": ...})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae44647-be92-4e28-9474-36826f84d548",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc1d5af-3436-4276-ac2a-5c79a15dece7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The core idea of agents is to use a language model to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8935942c-6328-4ae4-8b18-23791a1eab50",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"display: flex;  height: 260px;\">\n",
    "    <img src=\"resources/llm_agent.png\"  style=\"margin-left:auto; margin-right:auto\"/>\n",
    "    <img src=\"resources/langgraph.png\"  style=\"margin-left:auto; margin-right:auto\"/>\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4248ba4-0d3f-4cc6-a50e-64d14460b788",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Let's create a sample agent using ***LangGraph***:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10a4d96-2ece-4aff-8f51-755d1aba3606",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "LangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of (and intended to be used with) LangChain. It extends the ***LangChain Expression Language*** with the ability to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. It is inspired by Pregel and Apache Beam. The current interface exposed is one inspired by ***NetworkX***.\n",
    "\n",
    "\n",
    "**Our example task here is to create an agent that has access to wikipedia as an extended source of knowledge. The agent will search over the website if it needs an external source to answer the input queries.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959046c8-36af-4e42-977c-8e321567ba82",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. We will first define the tools we want to use. For this simple example, we will use a Langchain built-in search tool for wikipeida. However, it is really easy to create your own tools and they can be customized to different needs and scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c2e969-6df3-49af-af7c-d78f5f809921",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.tools.wikipedia.tool import WikipediaQueryRun, WikipediaAPIWrapper\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langgraph.prebuilt import ToolExecutor\n",
    "## Define the input schema for the tool function. In this case the tool accepts a query string as input.\n",
    "\n",
    "class Wikipeida_tool_arg_schema(BaseModel):\n",
    "    \"\"\"Input for the Wikipeida tool.\"\"\"\n",
    "\n",
    "    query: str = Field(description=\"search query to look up\")\n",
    "\n",
    "# A list of tools are provided for the agent to utilize\n",
    "tools = [WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(), args_schema=Wikipeida_tool_arg_schema)]\n",
    "\n",
    "# Wrap these tools in a simple LangGraph ToolExecutor a class that calls that tool, and returns the output\n",
    "tool_executor = ToolExecutor(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a7f6e-febc-44f0-ad0b-247f98806a80",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we need to load the chat model we want to use. Importantly, this should satisfy two criteria:\n",
    "\n",
    "1. It should work with lists of messages. We will represent all agent state in the form of messages, so it needs to be able to work well with them.\n",
    "\n",
    "2. It should work with the OpenAI function calling interface. This means it should either be an OpenAI model or a model that exposes a similar interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e3271-8971-45e6-929d-43e53dbbd36b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import format_tool_to_openai_function\n",
    "\n",
    "\n",
    "# We will set streaming=True so that we can stream tokens\n",
    "model = ChatOpenAI(temperature=0, streaming=True)\n",
    "\n",
    "# Converting the LangChain tools into the format for OpenAI function calling, and then bind them to the model class.\n",
    "functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "model_with_functions = model.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c69c5-d36e-47af-b7b4-6e8449aea8db",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Graph creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7623f1-e03d-46aa-9d50-55149d72ebd2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"display: flex;  height: 552px;\">\n",
    "    <img src=\"resources/langgraph_agent.png\"  style=\"margin-left:auto; margin-right:auto\"/>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8687436-28ea-483a-886f-465af85afab6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Agent state definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6de949d-8aef-415a-a0b4-c287e1b2ab12",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The main type of graph in `langgraph` is the `StatefulGraph`. This graph is parameterized by a state object that it passes around to each node. Each node then returns operations to update that state. These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute. Whether to set or add is denoted by annotating the state object you construct the graph with.\n",
    "\n",
    "For this example, the state we will track will just be a list of messages. We want each node to just add messages to that list. Therefore, we will use a `TypedDict` with one key (`messages`) and annotate it so that the `messages` attribute is always added to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bfa7e-d2e7-4ba5-a552-73cb40f3fdb9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "# Define the state properties that will be passed through the graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff511fd-9ca6-4ccd-b537-cd78bd6088f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define the node functions of the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608c9fd-0aa8-4b26-a487-3e7a40a07cac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now need to define a few different nodes in our graph. In `LangGraph`, a node can be either a function or a runnable. There are two main nodes we need for this:\n",
    "\n",
    "1. ***The agent***: responsible for deciding what (if any) actions to take.\n",
    "2. ***A function***: to invoke tools: if the agent decides to take an action, this node will then execute that action.\n",
    "\n",
    "Let's define the nodes, as well as a function that is defined inside the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7d2bbc-259f-46c8-bc57-b6e4ebcfe4b1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state):\n",
    "    messages = state['messages']\n",
    "    response = model_with_functions.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Define the function to execute tools\n",
    "def call_tool(state):\n",
    "    messages = state['messages']\n",
    "    # Based on the continue condition\n",
    "    # we know the last message involves a function call\n",
    "    last_message = messages[-1]\n",
    "    # We construct an ToolInvocation from the function_call\n",
    "    action = ToolInvocation(\n",
    "        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
    "        tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"]),\n",
    "    )\n",
    "    print(action)\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(content=str(response), name=action.tool)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e089a859-a2b3-4852-93ae-a09ffdef54de",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Connect the nodes and create the structure of the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2feffa-3a82-4222-883d-a04d70cea618",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will also need to define some edges. Some of these edges may be conditional. The reason they are conditional is that based on the output of a node, one of several paths may be taken. The path that is taken is not known until that node is run (the LLM decides).\n",
    "\n",
    "1. ***Conditional Edge***: after the agent is called, we should either: **a.** If the agent said to take an action, then the function to invoke tools should be called **b.** If the agent said that it was finished, then it should finish\n",
    "\n",
    "2. ***Normal Edge***: after the tools are invoked, it should always go back to the agent to decide what to do next\n",
    " \n",
    "Let's now connect the nodes defined above with proper edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74240f8-46ba-483e-93aa-003b03830578",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# Define the function that determines the route in the conditional edge\n",
    "def should_continue(state):\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if \"function_call\" not in last_message.additional_kwargs:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # We set 'agent' as the starting Node.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings output from the above function, and the values are other nodes' names.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `should_continue`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"continue\": \"action\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge('action', 'agent')\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would any other runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd8ef30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The graph structure can also be visualized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6b6db",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(app.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a718355e-7b0e-443f-992b-c300b1b458a4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The graph structure is compelete and now we can invoke it using input messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9943eb6-4d76-46e2-9e2d-c4e0a231623e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Hello how are you?\")]}\n",
    "for output in app.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb278a2-6e81-48df-8899-fb11775d49a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"What is langchain?\")]}\n",
    "for output in app.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b700f6-4b47-411f-a03c-d2bd9ae2a1fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3. Setting up LangServe as a REST API server for your agent\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <span style=\"font-size: 80px;\">🦜️🏓 LangServe</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438e8cd-4918-4d66-9062-4bc2b5860385",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "[LangServe](https://github.com/langchain-ai/langserve) helps developers\n",
    "deploy `LangChain` [runnables and chains](https://python.langchain.com/docs/expression_language/)\n",
    "as a REST API.\n",
    "\n",
    "This library is integrated with [FastAPI](https://fastapi.tiangolo.com/) and uses [pydantic](https://docs.pydantic.dev/latest/) for data validation.\n",
    "\n",
    "In addition, it provides a client that can be used to call into runnables deployed on a server.\n",
    "\n",
    "\n",
    "## Features\n",
    "\n",
    "- Input and Output schemas automatically inferred from your LangChain object, and\n",
    "  enforced on every API call, with rich error messages\n",
    "- API docs page with JSONSchema and Swagger (insert example link)\n",
    "- Efficient `/invoke/`, `/batch/` and `/stream/` endpoints with support for many\n",
    "  concurrent requests on a single server\n",
    "- `/stream_log/` endpoint for streaming all (or some) intermediate steps from your\n",
    "  chain/agent\n",
    "- **new** as of 0.0.40, supports `astream_events` to make it easier to stream without needing to parse the output of `stream_log`.\n",
    "- Playground page at `/playground/` with streaming output and intermediate steps\n",
    "- Built-in (optional) tracing to [LangSmith](https://www.langchain.com/langsmith), just\n",
    "  add your API key (see [Instructions](https://docs.smith.langchain.com/))\n",
    "- All built with battle-tested open-source Python libraries like FastAPI, Pydantic,\n",
    "  uvloop and asyncio.\n",
    "- Use the client SDK to call a LangServe server as if it was a Runnable running\n",
    "  locally (or call the HTTP API directly)\n",
    "- [LangServe Hub](https://github.com/langchain-ai/langchain/blob/master/templates/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58066310-e38f-4f93-9af6-e47560ecde32",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The main code for setting up the langserver server is in files titles \"server_..._.py\".\n",
    "You can look at the code for server of the agent defined above with single table query tools by uncommenting and running the code below or looking at the file ```server.py```.\n",
    "\n",
    "***Note*** that the code won't run from notebook due to ```asyncio``` issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19958f4-d416-4ad3-b136-739df110d78f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %load server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d87a325-f2f7-41a5-9319-30322f54597c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The server is started by running the command below. You can also run this command from the terminal.\n",
    "\n",
    "### Important: Please note the server port in the logs of this cell. It is written in the format :\n",
    "### http://0.0.0.0:<port_number\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b6bce5-50dd-4d9d-8d14-d2cbb1111934",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Define the command to run the server\n",
    "server_command = \"uvicorn server:app --host 0.0.0.0 --port 0 --reload\"\n",
    "\n",
    "# Start the server in a separate process\n",
    "server_process = subprocess.Popen(server_command.split())\n",
    "\n",
    "# Wait for a few seconds to ensure the server has started\n",
    "time.sleep(5)\n",
    "\n",
    "# Now you can continue executing code in your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac278cb8-3bf2-4836-be1d-7d5b2769b7a7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Paste the port number in the following code cell to run queries against the REST server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3204efca-6f28-432c-ac7a-d1e2adbb871a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(\n",
    "            content=\"What is LangChain\", ## write the question instead of three dots.\n",
    "            role='human'\n",
    "        )\n",
    "    ]\n",
    "}\n",
    "\n",
    "from langserve import RemoteRunnable\n",
    "\n",
    "# Paste the correct port number for the server instead of <port_number>\n",
    "openai_llm = RemoteRunnable(\"http://localhost:35885/openai/\") \n",
    "\n",
    "for msg in openai_llm.stream(inputs):\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f504515-160c-4b40-aee0-3e42328d67a9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### You can stop the server by running below cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d71f1-7219-433e-9471-6e8c491f4dc2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# To stop the server subprocess\n",
    "server_process.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb2738e-c4af-4ae4-862d-9ec5010632a8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4. TODO: Multi tool agent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99d82e-947f-44d5-a374-87279eec1ed0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Try to define a new tool and add it to the list of tools for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9fd69-6057-422c-beea-e5e01e875523",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import tool\n",
    "\n",
    "class CalculatorInput(BaseModel):\n",
    "    #TODO Define the schema of the input to the tool\n",
    "    a: ... = Field(description=\"...\") # TODO: fill in the blanks with proper object types and description of parameters for the first number\n",
    "    b: ... = Field(description=\"...\") # second number\n",
    "    op: ... = Field(description=\"...\") # operation type\n",
    "\n",
    "@tool(args_schema=CalculatorInput)\n",
    "def calculator_tool(a: ..., b: ..., op: ...) -> int:\n",
    "    \"\"\"...\"\"\" #fill in the description of the tool. This description helps the agent to decide when to use the tool based on the input prompt.\n",
    "    return ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8369bfd4-0411-4933-b411-5fbc76ce0613",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## TODO: add the defined tool above to the list of tools for the LangGraph agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec465e1d-f7a5-44f0-b0aa-7bf9f1ea644e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The aim is for the agent to have access to both tools of wikipedia search and the custom tool defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba8aeee-870f-4188-9170-a58bb2a12cbb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# A list of tools are provided for the agent to utilize\n",
    "tools = [...]\n",
    "\n",
    "# Wrap these tools in a simple LangGraph ToolExecutor a class that calls that tool, and returns the output\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "# Bind the language model to the tools defined above.\n",
    "# Use the code above in the agent definition section to get inspiration for this part. The aim is for the tools list to have 2 values :\n",
    "# 1. Wikipeida tool and 2. the custom calculator tool 3. Optional custom tool definition.\n",
    "model = ChatOpenAI(temperature=0, streaming=True)\n",
    "functions = [format_tool_to_openai_function(t) for t in tools]\n",
    "model_with_functions = model.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3cb2c-ca91-4ed1-8dd1-7b868ded884c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e60c93-e857-416a-90c6-4ad3e139a22f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Test out the complete code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccc4ddb-47e4-4719-993b-96fb2498bda8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\": [HumanMessage(content=\"What is 2375 devided by 452?\")]}\n",
    "for output in app.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d1d7c-9eba-4ff7-8bd7-2065b76e8386",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
